{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path where your CSV files are located\n",
    "path = 'dataset'  # Replace with your actual path\n",
    "\n",
    "# Use glob to find all CSV files in the directory\n",
    "csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: 002_lomo_sellers_dataset.csv\n",
      "  - No missing values found.\n",
      "  - No duplicate rows found.\n",
      "Processing dataset: 007_lomo_order_items_dataset.csv\n",
      "  - No missing values found.\n",
      "  - No duplicate rows found.\n",
      "Processing dataset: 008_lomo_order_payments_dataset.csv\n",
      "  - No missing values found.\n",
      "  - No duplicate rows found.\n",
      "Processing dataset: 006_lomo_orders_dataset.csv\n",
      "  - Contains 4908 missing values.\n",
      "  - No duplicate rows found.\n",
      "Processing dataset: 010_lomo_marketing_qualified_leads_dataset.csv\n",
      "  - No missing values found.\n",
      "  - No duplicate rows found.\n",
      "Processing dataset: 011_lomo_closed_deals_dataset.csv\n",
      "  - No missing values found.\n",
      "  - No duplicate rows found.\n",
      "Processing dataset: 003_lomo_geolocation_dataset.csv\n",
      "  - No missing values found.\n",
      "  - No duplicate rows found.\n",
      "Processing dataset: 009_lomo_order_reviews_dataset.csv\n",
      "  - Contains 146532 missing values.\n",
      "  - No duplicate rows found.\n",
      "Processing dataset: 001_lomo_customers_dataset.csv\n",
      "  - No missing values found.\n",
      "  - No duplicate rows found.\n",
      "Processing dataset: 004_lomo_products_dataset.csv\n",
      "  - No missing values found.\n",
      "  - No duplicate rows found.\n",
      "Processing dataset: 005_lomo_product_category_name_translation.csv\n",
      "  - No missing values found.\n",
      "  - No duplicate rows found.\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to hold individual dataframes with their file names as keys\n",
    "dataframes = {}\n",
    "\n",
    "# Loop through the list of CSV files and read each file into a separate dataframe\n",
    "for file in csv_files:\n",
    "    # Extract the file name (without path) to use as a key\n",
    "    file_name = os.path.basename(file)\n",
    "    \n",
    "    # Read the file into a DataFrame\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # Print the name of the dataset\n",
    "    print(f\"Processing dataset: {file_name}\")\n",
    "    \n",
    "    # Check for missing values (NaNs)\n",
    "    if df.isna().sum().sum() > 0:\n",
    "        print(f\"  - Contains {df.isna().sum().sum()} missing values.\")\n",
    "    else:\n",
    "        print(\"  - No missing values found.\")\n",
    "    \n",
    "    # Check for duplicate rows\n",
    "    if df.duplicated().sum() > 0:\n",
    "        print(f\"  - Contains {df.duplicated().sum()} duplicate rows.\")\n",
    "    else:\n",
    "        print(\"  - No duplicate rows found.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Table 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with NaN values: ['order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date']\n"
     ]
    }
   ],
   "source": [
    "# Check for columns that have NaN values\n",
    "columns_with_na = df_006.columns[df_006.isna().any()].tolist()\n",
    "\n",
    "\n",
    "# Print out the columns that contain NaN values\n",
    "print(\"Columns with NaN values:\", columns_with_na)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             order_id                       customer_id  \\\n",
      "6    136cce7faa42fdb2cefd53fdc79a6098  ed0271e0b7da060a393796590e7b737a   \n",
      "44   ee64d42b8cf066f35eac1cf57de1aa85  caded193e8e47b8362864762a83db3c5   \n",
      "103  0760a852e4e9d89eb77bf631eaaf1c84  d2a79636084590b7465af8ab374a8cf5   \n",
      "128  15bed8e2fec7fdbadb186b57c46c92f2  f3f0e613e0bdb9c7cee75504f0f90679   \n",
      "154  6942b8da583c2f9957e990d028607019  52006a9383bf149a4fb24226b173106f   \n",
      "162  36530871a5e80138db53bcfd8a104d90  4dafe3c841d2d6cc8a8b6d25b35704b9   \n",
      "231  4d630f57194f5aba1a3d12ce23e71cd9  6d491c9fe2f04f6e2af6ec033cd8907c   \n",
      "266  8e24261a7e58791d10cb1bf9da94df5c  64a254d30eed42cd0e6c36dddb88adf0   \n",
      "299  3b4ad687e7e5190db827e1ae5a8989dd  1a87b8517b7d31373b50396eb15cb445   \n",
      "305  b68d69564a79dea4776afa33d1d2fcab  de1e5517fb50896bbdcff5814fb31802   \n",
      "\n",
      "    order_status order_purchase_timestamp order_approved_at  \\\n",
      "6       invoiced         11/04/2017 12:22  13/04/2017 13:25   \n",
      "44       shipped         04/06/2018 16:44  05/06/2018 04:31   \n",
      "103     invoiced         03/08/2018 17:44  07/08/2018 06:15   \n",
      "128   processing         03/09/2017 14:22  03/09/2017 14:30   \n",
      "154      shipped         10/01/2018 11:33  11/01/2018 02:32   \n",
      "162      shipped         09/05/2017 11:48  11/05/2017 11:45   \n",
      "231      shipped         17/11/2017 19:53  18/11/2017 19:50   \n",
      "266  unavailable         16/11/2017 15:09  16/11/2017 15:26   \n",
      "299      shipped         28/06/2018 12:52  28/06/2018 13:11   \n",
      "305      shipped         28/02/2018 08:57  28/02/2018 10:40   \n",
      "\n",
      "    order_delivered_carrier_date order_delivered_customer_date  \\\n",
      "6                            NaN                           NaN   \n",
      "44              05/06/2018 14:32                           NaN   \n",
      "103                          NaN                           NaN   \n",
      "128                          NaN                           NaN   \n",
      "154             11/01/2018 19:39                           NaN   \n",
      "162             11/05/2017 13:21                           NaN   \n",
      "231             22/11/2017 17:28                           NaN   \n",
      "266                          NaN                           NaN   \n",
      "299             04/07/2018 15:20                           NaN   \n",
      "305             05/03/2018 16:10                           NaN   \n",
      "\n",
      "    order_estimated_delivery_date  \n",
      "6                09/05/2017 00:00  \n",
      "44               28/06/2018 00:00  \n",
      "103              21/08/2018 00:00  \n",
      "128              03/10/2017 00:00  \n",
      "154              07/02/2018 00:00  \n",
      "162              08/06/2017 00:00  \n",
      "231              13/12/2017 00:00  \n",
      "266              05/12/2017 00:00  \n",
      "299              03/08/2018 00:00  \n",
      "305              23/03/2018 00:00  \n"
     ]
    }
   ],
   "source": [
    "# check Table 6 \n",
    "df_006 = pd.read_csv('/Users/shanelim/Desktop/Sem 3.1/Applications/PWC/data-analytics-case-study-dataset/006_lomo_orders_dataset.csv', encoding='utf-8')\n",
    "\n",
    "# Filter the rows that contain NaN values\n",
    "rows_with_na = df_006[df_006.isna().any(axis=1)]\n",
    "\n",
    "# Display the first 10 rows with NaN values\n",
    "print(rows_with_na.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_status\n",
      "approved          2\n",
      "canceled        619\n",
      "created           5\n",
      "delivered        23\n",
      "invoiced        314\n",
      "processing      301\n",
      "shipped        1107\n",
      "unavailable     609\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "na_count_by_order_status = rows_with_na.groupby('order_status').size()\n",
    "\n",
    "# Display the result\n",
    "print(na_count_by_order_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Table 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check Table 9 - order reviews\n",
    "df_009 = pd.read_csv('dataset.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with NaN values: ['review_comment_title', 'review_comment_message']\n"
     ]
    }
   ],
   "source": [
    "# Check for columns that have NaN values\n",
    "columns_with_na = df_009.columns[df_009.isna().any()].tolist()\n",
    "\n",
    "\n",
    "# Print out the columns that contain NaN values\n",
    "print(\"Columns with NaN values:\", columns_with_na)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           review_id                          order_id  \\\n",
      "0   7bc2406110b926393aa56f80a40eba40  73fc7af87114b39712e6da79b0a377eb   \n",
      "1   80e641a11e56f04c1ad469d5645fdfde  a548910a1c6147796b98fdf73dbeba33   \n",
      "2   228ce5500dc1d8e020d8d1322874b6f0  f9e4b658b201a9f2ecdecbb34bed034b   \n",
      "3   e64fb393e7b32834bb789ff8bb30750e  658677c97b385a9be170737859d3511b   \n",
      "4   f7c4243c7fe1938f181bec41a392bdeb  8e6bfb81e283fa7e4f11123a3fb894f1   \n",
      "5   15197aa66ff4d0650b5434f1b46cda19  b18dcdf73be66366873cd26c5724d1dc   \n",
      "6   07f9bee5d1b850860defd761afa7ff16  e48aa0d2dcec3a2e87348811bcfdf22b   \n",
      "7   7c6400515c67679fbee952a7525281ef  c31a859e34e3adac22f376954e19b39d   \n",
      "8   a3f6f7f6f433de0aefbb97da197c554c  9c214ac970e84273583ab523dfafd09b   \n",
      "10  c9cfd2d5ab5911836ababae136c3a10c  cdf9aa68e72324eeb25c7de974696ee2   \n",
      "\n",
      "    review_score review_comment_title  \\\n",
      "0              4                  NaN   \n",
      "1              5                  NaN   \n",
      "2              5                  NaN   \n",
      "3              5                  NaN   \n",
      "4              5                  NaN   \n",
      "5              1                  NaN   \n",
      "6              5                  NaN   \n",
      "7              5                  NaN   \n",
      "8              5                  NaN   \n",
      "10             5                  NaN   \n",
      "\n",
      "                               review_comment_message review_creation_date  \\\n",
      "0                                                 NaN     18/01/2018 00:00   \n",
      "1                                                 NaN     10/03/2018 00:00   \n",
      "2                                                 NaN     17/02/2018 00:00   \n",
      "3               Recebi bem antes do prazo estipulado.     21/04/2017 00:00   \n",
      "4   ParabÃ©ns lojas lannister adorei comprar pela ...     01/03/2018 00:00   \n",
      "5                                                 NaN     13/04/2018 00:00   \n",
      "6                                                 NaN     16/07/2017 00:00   \n",
      "7                                                 NaN     14/08/2018 00:00   \n",
      "8                                                 NaN     17/05/2017 00:00   \n",
      "10                                                NaN     23/12/2017 00:00   \n",
      "\n",
      "   review_answer_timestamp  \n",
      "0         18/01/2018 21:46  \n",
      "1         11/03/2018 03:05  \n",
      "2         18/02/2018 14:36  \n",
      "3         21/04/2017 22:02  \n",
      "4         02/03/2018 10:26  \n",
      "5         16/04/2018 00:39  \n",
      "6         18/07/2017 19:30  \n",
      "7         14/08/2018 21:36  \n",
      "8         18/05/2017 12:05  \n",
      "10        26/12/2017 14:36  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Filter the rows that contain NaN values\n",
    "rows_with_na = df_009[df_009.isna().any(axis=1)]\n",
    "\n",
    "# Display the first 10 rows with NaN values\n",
    "print(rows_with_na.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: 002_lomo_sellers_dataset.csv\n",
      "  - No missing values found.\n",
      "  - No duplicate rows found.\n",
      "  - Finished cleaning dataset: 002_lomo_sellers_dataset.csv\n",
      "\n",
      "Processing dataset: 007_lomo_order_items_dataset.csv\n",
      "  - No missing values found.\n",
      "  - No duplicate rows found.\n",
      "  - Finished cleaning dataset: 007_lomo_order_items_dataset.csv\n",
      "\n",
      "Processing dataset: 008_lomo_order_payments_dataset.csv\n",
      "  - No missing values found.\n",
      "  - No duplicate rows found.\n",
      "  - Finished cleaning dataset: 008_lomo_order_payments_dataset.csv\n",
      "\n",
      "Processing dataset: 006_lomo_orders_dataset.csv\n",
      "  - Contains 4908 missing values.\n",
      "  - No duplicate rows found.\n",
      "  - Finished cleaning dataset: 006_lomo_orders_dataset.csv\n",
      "\n",
      "Processing dataset: 010_lomo_marketing_qualified_leads_dataset.csv\n",
      "  - No missing values found.\n",
      "  - No duplicate rows found.\n",
      "  - Finished cleaning dataset: 010_lomo_marketing_qualified_leads_dataset.csv\n",
      "\n",
      "Processing dataset: 011_lomo_closed_deals_dataset.csv\n",
      "  - No missing values found.\n",
      "  - No duplicate rows found.\n",
      "  - Finished cleaning dataset: 011_lomo_closed_deals_dataset.csv\n",
      "\n",
      "Processing dataset: 003_lomo_geolocation_dataset.csv\n",
      "  - No missing values found.\n",
      "  - No duplicate rows found.\n",
      "  - Finished cleaning dataset: 003_lomo_geolocation_dataset.csv\n",
      "\n",
      "Processing dataset: 009_lomo_order_reviews_dataset.csv\n",
      "  - Contains 146532 missing values.\n",
      "  - No duplicate rows found.\n",
      "  - Finished cleaning dataset: 009_lomo_order_reviews_dataset.csv\n",
      "\n",
      "Processing dataset: 001_lomo_customers_dataset.csv\n",
      "  - No missing values found.\n",
      "  - No duplicate rows found.\n",
      "  - Finished cleaning dataset: 001_lomo_customers_dataset.csv\n",
      "\n",
      "Processing dataset: 004_lomo_products_dataset.csv\n",
      "  - No missing values found.\n",
      "  - No duplicate rows found.\n",
      "  - Finished cleaning dataset: 004_lomo_products_dataset.csv\n",
      "\n",
      "Processing dataset: 005_lomo_product_category_name_translation.csv\n",
      "  - No missing values found.\n",
      "  - No duplicate rows found.\n",
      "  - Finished cleaning dataset: 005_lomo_product_category_name_translation.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to hold individual dataframes with their file names as keys\n",
    "dataframes = {}\n",
    "\n",
    "# Loop through the list of CSV files and read each file into a separate dataframe\n",
    "for file in csv_files:\n",
    "    # Extract the file name (without path) to use as a key\n",
    "    file_name = os.path.basename(file)\n",
    "    \n",
    "    # Read the file into a DataFrame\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # Print the name of the dataset\n",
    "    print(f\"Processing dataset: {file_name}\")\n",
    "    \n",
    "    # Check for missing values (NaNs)\n",
    "    if df.isna().sum().sum() > 0:\n",
    "        print(f\"  - Contains {df.isna().sum().sum()} missing values.\")\n",
    "    else:\n",
    "        print(\"  - No missing values found.\")\n",
    "    \n",
    "    # Check for duplicate rows\n",
    "    if df.duplicated().sum() > 0:\n",
    "        print(f\"  - Contains {df.duplicated().sum()} duplicate rows.\")\n",
    "    else:\n",
    "        print(\"  - No duplicate rows found.\")\n",
    "    \n",
    "    # Cleaning Step 1: Remove rows with missing values\n",
    "    df_cleaned = df.dropna()\n",
    "\n",
    "    # Cleaning Step 2: Remove duplicate rows\n",
    "    df_cleaned = df_cleaned.drop_duplicates()\n",
    "\n",
    "    # Cleaning Step 3: Standardize column names\n",
    "    df_cleaned.columns = df_cleaned.columns.str.lower().str.strip()\n",
    "\n",
    "    # Cleaning Step 4: Remove leading/trailing whitespace from string columns\n",
    "    for col in df_cleaned.select_dtypes(include=['object']).columns:\n",
    "        df_cleaned[col] = df_cleaned[col].str.strip()\n",
    "    \n",
    "    # Store the cleaned DataFrame in the dictionary\n",
    "    dataframes[file_name] = df_cleaned\n",
    "\n",
    "    # Print completion for current dataset\n",
    "    print(f\"  - Finished cleaning dataset: {file_name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned file: cleaned_002_lomo_sellers_dataset.csv\n",
      "Saved cleaned file: cleaned_007_lomo_order_items_dataset.csv\n",
      "Saved cleaned file: cleaned_008_lomo_order_payments_dataset.csv\n",
      "Saved cleaned file: cleaned_006_lomo_orders_dataset.csv\n",
      "Saved cleaned file: cleaned_010_lomo_marketing_qualified_leads_dataset.csv\n",
      "Saved cleaned file: cleaned_011_lomo_closed_deals_dataset.csv\n",
      "Saved cleaned file: cleaned_003_lomo_geolocation_dataset.csv\n",
      "Saved cleaned file: cleaned_009_lomo_order_reviews_dataset.csv\n",
      "Saved cleaned file: cleaned_001_lomo_customers_dataset.csv\n",
      "Saved cleaned file: cleaned_004_lomo_products_dataset.csv\n",
      "Saved cleaned file: cleaned_005_lomo_product_category_name_translation.csv\n"
     ]
    }
   ],
   "source": [
    "# Save each cleaned DataFrame to a new CSV file\n",
    "for file_name, df_cleaned in dataframes.items():\n",
    "    cleaned_file_name = f\"cleaned_{file_name}\"\n",
    "    df_cleaned.to_csv(cleaned_file_name, index=False)\n",
    "    print(f\"Saved cleaned file: {cleaned_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   object_id    shape admin0_name_en admin0_name_th admin0_pcode  \\\n",
      "0          1  Polygon       Thailand      ประเทศไทย           TH   \n",
      "1          2  Polygon       Thailand      ประเทศไทย           TH   \n",
      "2          3  Polygon       Thailand      ประเทศไทย           TH   \n",
      "3          4  Polygon       Thailand      ประเทศไทย           TH   \n",
      "4          5  Polygon       Thailand      ประเทศไทย           TH   \n",
      "\n",
      "  admin1_name_en admin1_name_th admin1_pcode  admin2_name_en admin2_name_th  \\\n",
      "0           Loei            เลย         TH42          Tha Li         ท่าลี่   \n",
      "1        Pattani        ปัตตานี         TH94  Mueang Pattani   เมืองปัตตานี   \n",
      "2          Surin       สุรินทร์         TH32         Buachet         บัวเชด   \n",
      "3           Yala           ยะลา         TH95           Raman          รามัน   \n",
      "4            Nan           น่าน         TH55        Wiang Sa        เวียงสา   \n",
      "\n",
      "  admin2_pcode admin3_name_en admin3_name_th admin3_pcode  shape_length  \\\n",
      "0       TH4208           A Hi           อาฮี     TH420803      0.360041   \n",
      "1       TH9401         A Noru       อาเนาะรู     TH940102      0.044900   \n",
      "2       TH3213         A Phon          อาโพน     TH321305      0.365838   \n",
      "3       TH9506         A Song         อาซ่อง     TH950615      0.314589   \n",
      "4       TH5507      Ai Na Lai      อ่ายนาไลย     TH550711      1.102764   \n",
      "\n",
      "   shape_area  \n",
      "0    0.005078  \n",
      "1    0.000120  \n",
      "2    0.004086  \n",
      "3    0.003177  \n",
      "4    0.014955  \n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file with UTF-8 encoding to support Thai characters\n",
    "df = pd.read_csv('dataset.csv', encoding='utf-8')\n",
    "\n",
    "# Check the first few rows to ensure it loaded correctly\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          review_id                          order_id  \\\n",
      "0  8670d52e15e00043ae7de4c01cc2fe06  b9bf720beb4ab3728760088589c62129   \n",
      "1  3948b09f7c818e2d86c9a546758b2335  e51478e7e277a83743b6f9991dbfa3fb   \n",
      "2  373cbeecea8286a2b66c97b1b157ec46  583174fbe37d3d5f0d6661be3aad1786   \n",
      "3  d21bbc789670eab777d27372ab9094cc  4fc44d78867142c627497b60a7e0228a   \n",
      "4  c92cdd7dd544a01aa35137f901669cdf  37e7875cdce5a9e5b3a692971f370151   \n",
      "\n",
      "   review_score     review_comment_title  \\\n",
      "0             4                recomendo   \n",
      "1             5          Super recomendo   \n",
      "2             1  NÃ£o chegou meu produto   \n",
      "3             5                   Ã“timo   \n",
      "4             4               Muito bom.   \n",
      "\n",
      "                              review_comment_message review_creation_date  \\\n",
      "0  aparelho eficiente. no site a marca do aparelh...     22/05/2018 00:00   \n",
      "1  Vendedor confiÃ¡vel, produto ok e entrega ante...     23/05/2018 00:00   \n",
      "2                                           PÃ©ssimo     15/08/2018 00:00   \n",
      "3                                       Loja nota 10     10/07/2018 00:00   \n",
      "4  Recebi exatamente o que esperava. As demais en...     07/06/2018 00:00   \n",
      "\n",
      "  review_answer_timestamp  \n",
      "0        23/05/2018 16:45  \n",
      "1        24/05/2018 03:00  \n",
      "2        15/08/2018 04:10  \n",
      "3        11/07/2018 14:10  \n",
      "4        09/06/2018 18:44  \n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file with UTF-8 encoding to support Thai characters\n",
    "df_009 = pd.read_csv('dataset.csv', encoding='utf-8')\n",
    "\n",
    "# Check the first few rows to ensure it loaded correctly\n",
    "print(df_009.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
